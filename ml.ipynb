{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hologram Optimization and Phase Mask Prediction Notebook\n",
    "\n",
    "This notebook provides a complete workflow to:\n",
    "\n",
    "1. **Generate Phase Masks:** For each image in a folder (e.g., `50pngs`), an iterative TensorFlow optimization process creates a phase mask. The phase masks are cropped and normalized to [0, 1] and then cached.\n",
    "2. **Build Dataset for Training:** The input images are used to build a multi-channel (intensity) stack and paired with their corresponding phase mask (a 2D array) as the target output.\n",
    "3. **Train a PyTorch Model:** A simple U-Net model is trained to map the multi-channel input to the phase mask.\n",
    "4. **Evaluate and Visualize Predictions:** The notebook displays the input image, the target phase mask, and the model’s predicted phase mask side-by-side.\n",
    "\n",
    "Each cell is organized so you can run the entire workflow sequentially without having to rerun individual sections over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# PART A: IMPORTS & ITERATIVE CODE\n",
    "# -------------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# -- TensorFlow (for iterative optimization)\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# -- PyTorch (for model training)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "def compute_phase_mask_iterative(input_img_path: str,\n",
    "                                 size_holo=1600,\n",
    "                                 size_pad=300,\n",
    "                                 num_layers_z=4,\n",
    "                                 learning_rate=0.003,\n",
    "                                 train_epochs=10):\n",
    "    \"\"\"\n",
    "    This function performs iterative optimization to generate a phase mask.\n",
    "    It:\n",
    "      1) Loads an ideal intensity target image from input_img_path.\n",
    "      2) Resizes it to (size_padded, size_padded) and replicates it across num_layers_z slices.\n",
    "      3) Uses an Angular Spectrum Method (ASM) to propagate a phase mask.\n",
    "      4) Optimizes the phase mask (initialized randomly) so that the propagated field matches the intensity target.\n",
    "      5) Returns the optimized phase mask (cropped to the hologram region and normalized to [0,1]).\n",
    "    \n",
    "    The returned phase mask is a 2D array.\n",
    "    \"\"\"\n",
    "    print(f\"[Iterative] Processing image: {input_img_path}\")\n",
    "    # Setup parameters\n",
    "    nm = 1e-9; um = 1e-6; mm = 1e-3\n",
    "    lamb = 405 * nm            # wavelength (m)\n",
    "    pixel_size = 1.5 * um\n",
    "    size_padded = size_holo + 2 * size_pad\n",
    "\n",
    "    # Load and resize target image (ideal intensity) and normalize to [0,1]\n",
    "    img_pil = Image.open(input_img_path).convert('L')\n",
    "    img_pil = img_pil.resize((size_padded, size_padded), Image.BICUBIC)\n",
    "    img_np = np.array(img_pil, dtype=np.float32) / 255.0\n",
    "\n",
    "    # Replicate the image over num_layers_z slices; we use one slice (e.g., the first) for loss.\n",
    "    holo_target = np.stack([img_np] * num_layers_z, axis=-1)  # shape: (H, W, num_layers_z)\n",
    "\n",
    "    # --------- ASM Preparation ---------\n",
    "    dfx = 1 / (size_padded * pixel_size)\n",
    "    dfy = 1 / (size_padded * pixel_size)\n",
    "    k_x = dfx * (np.linspace(-size_padded/2, size_padded/2 - 1, size_padded))\n",
    "    k_y = dfy * (np.linspace(-size_padded/2, size_padded/2 - 1, size_padded))\n",
    "    fx, fy = np.meshgrid(k_x, -k_y)\n",
    "    p = (fx**2 + fy**2) * (lamb**2)\n",
    "    p = p.astype(np.float32)\n",
    "\n",
    "    def asm_gpu(ob, z, lam, mx, my, pixel_size):\n",
    "        # Precompute p as a TensorFlow constant\n",
    "        p_tf = tf.constant(p)\n",
    "        sp = tf.sqrt(1 - p_tf)\n",
    "        sp = tf.cast(sp, tf.complex64)\n",
    "        factor = 2 * np.pi * 1j * z / lam\n",
    "        # q is the propagation kernel (transfer function)\n",
    "        q = tf.signal.fftshift(tf.exp(factor * sp))\n",
    "        return tf.ifft2d(tf.fft2d(ob) * q)\n",
    "\n",
    "    # --------- Build TensorFlow Graph ---------\n",
    "    # x1: z-plane position (a single value wrapped as a 1-element vector)\n",
    "    x1 = tf.placeholder(tf.complex64, [1])\n",
    "    # x2: target intensity image for one slice, size: (size_padded, size_padded)\n",
    "    x2 = tf.placeholder(tf.float32, [size_padded, size_padded])\n",
    "    # Random initialization of phase mask (2D, shape: [size_padded, size_padded])\n",
    "    phase_init = tf.random_normal([size_padded, size_padded])\n",
    "    phase_mask = tf.Variable(phase_init, dtype=tf.float32)\n",
    "    # Enforce phase values into [0, 2π) using modulo arithmetic:\n",
    "    phase_mask = 2 * np.pi * (tf.mod(phase_mask, 1))\n",
    "    # Convert phase mask into a complex field (magnitude = 1)\n",
    "    phase_mask_complex = tf.exp(1j * tf.cast(phase_mask, tf.complex64))\n",
    "    # Propagate through ASM to get simulated intensity (hotplot)\n",
    "    holo_prop = tf.abs(asm_gpu(phase_mask_complex, x1, lamb, size_padded, size_padded, pixel_size)) ** 2\n",
    "    # Loss compares the propagated intensity with the target intensity (for one slice)\n",
    "    loss = tf.reduce_mean(tf.square(holo_prop - x2))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Choose a propagation distance z (here 5mm)\n",
    "    z_val = np.array([5 * mm], dtype=np.complex64)\n",
    "\n",
    "    # Iterative optimization loop\n",
    "    for epoch in range(train_epochs):\n",
    "        _, cur_loss = sess.run([optimizer, loss], feed_dict={\n",
    "            x1: z_val,\n",
    "            x2: holo_target[:, :, 0]  # using the first slice as the target intensity\n",
    "        })\n",
    "        print(f\"[Iterative] Epoch {epoch+1}/{train_epochs}, loss = {cur_loss:.6f}\")\n",
    "\n",
    "    # Retrieve the optimized phase mask (values in [0, 2π])\n",
    "    final_phase = sess.run(phase_mask)\n",
    "    sess.close()\n",
    "    # Crop out the hologram region by removing padding (resulting in a 2D array)\n",
    "    final_phase_cropped = final_phase[size_pad:size_pad+size_holo, size_pad:size_pad+size_holo]\n",
    "    # Normalize the phase mask to [0,1] by dividing by 2π\n",
    "    final_phase_norm = final_phase_cropped / (2 * np.pi)\n",
    "    print(f\"[Iterative] Finished processing {input_img_path}\")\n",
    "    return final_phase_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we define the PyTorch dataset that reads images from a folder, uses the above function to either compute or load the cached phase mask, and prepares the input-output pair for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# PART B: DATASET & TRAINING IN PYTORCH\n",
    "# -------------------------------\n",
    "class HotplotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that:\n",
    "      1) Reads each ideal target image from a folder.\n",
    "      2) Checks if a cached phase mask (.npy file) exists; if not, computes it.\n",
    "      3) Creates a multi-channel input by replicating the image over num_layers_z channels.\n",
    "      4) Returns (input_intensity_stack, phase_mask) as tensors.\n",
    "         - Input: shape (num_layers_z, H, W)\n",
    "         - Target: shape (1, H, W), the optimized phase mask (normalized to [0,1])\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, cache_dir=\"cache_hotplots\", num_layers_z=4, augment=True):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.num_layers_z = num_layers_z\n",
    "        self.augment = augment\n",
    "        if not os.path.exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "            print(f\"[Cache] Created cache directory at {self.cache_dir}\")\n",
    "        self.img_files = sorted([\n",
    "            f for f in os.listdir(img_dir)\n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))\n",
    "        ])\n",
    "        print(f\"[Dataset] Found {len(self.img_files)} images in {img_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        cache_file = os.path.join(self.cache_dir, img_name + \"_phase.npy\")\n",
    "\n",
    "        # Load the ideal intensity target image\n",
    "        img_pil = Image.open(img_path).convert('L')\n",
    "        img_np = np.array(img_pil, dtype=np.float32) / 255.0\n",
    "\n",
    "        # Create multi-channel input by replicating over num_layers_z channels\n",
    "        input_stack = np.stack([img_np] * self.num_layers_z, axis=-1)  # shape (H, W, num_layers_z)\n",
    "        input_stack = np.transpose(input_stack, (2, 0, 1))  # rearrange to (num_layers_z, H, W)\n",
    "\n",
    "        # Check if phase mask is cached; if not, compute it using iterative optimization.\n",
    "        if os.path.exists(cache_file):\n",
    "            print(f\"[Cache] Loading cached phase mask for {img_name}\")\n",
    "            phase_mask_np = np.load(cache_file)\n",
    "        else:\n",
    "            print(f\"[Cache] Computing phase mask for {img_name}\")\n",
    "            phase_mask_np = compute_phase_mask_iterative(img_path, train_epochs=3)\n",
    "            print(f\"[Cache] Saving phase mask for {img_name} to {cache_file}\")\n",
    "            np.save(cache_file, phase_mask_np)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        x_tensor = torch.from_numpy(input_stack).float()      # shape: (num_layers_z, H, W)\n",
    "        y_tensor = torch.from_numpy(phase_mask_np.astype(np.float32)).unsqueeze(0)  # shape: (1, H, W)\n",
    "\n",
    "        # Optionally apply data augmentation (random flips)\n",
    "        if self.augment:\n",
    "            x_tensor, y_tensor = self.random_flip(x_tensor, y_tensor)\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "    def random_flip(self, x, y):\n",
    "        if np.random.rand() < 0.5:\n",
    "            x = torch.flip(x, dims=[2])\n",
    "            y = torch.flip(y, dims=[2])\n",
    "        if np.random.rand() < 0.5:\n",
    "            x = torch.flip(x, dims=[1])\n",
    "            y = torch.flip(y, dims=[1])\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple U-Net model that maps the multi-channel intensity input to a single-channel phase mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=1, features=32):\n",
    "        super(SimpleUNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, features, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(features, features, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv3 = nn.Conv2d(features, features, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(features, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.up(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.conv4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the main function to train, evaluate, and visualize the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1) CREATE DATASET\n",
    "    input_dir = r\"datasets/50pngs\"  # Folder with ~50 training images; adjust path as needed\n",
    "    num_layers_z = 4  # Number of slices (channels) for the input stack\n",
    "    dataset = HotplotDataset(input_dir, cache_dir=\"cache_hotplots\", num_layers_z=num_layers_z, augment=True)\n",
    "    print(f\"[Main] Found {len(dataset)} images.\")\n",
    "\n",
    "    # Train-test split (80/20)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    print(f\"[Main] Training on {train_size} images, testing on {test_size} images.\")\n",
    "\n",
    "    # Dataloaders\n",
    "    batch_size = 1  # Small batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 2) CREATE MODEL & OPTIMIZER\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SimpleUNet(in_channels=num_layers_z, out_channels=1, features=32).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # 3) TRAIN\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f\"[Train] Starting epoch {epoch+1}/{num_epochs}...\")\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print(f\"[Train] Epoch {epoch+1}, sample {i+1}/{len(train_loader)}: Loss = {loss.item():.6f}\")\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"[Train] Epoch {epoch+1} finished with average loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # 4) TEST\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    print(\"[Test] Starting evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            print(f\"[Test] Sample {i+1}/{len(test_loader)}: Loss = {loss.item():.6f}\")\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"[Test] Final Test MSE Loss: {test_loss:.6f}\")\n",
    "\n",
    "    # 5) VISUALIZE PREDICTIONS\n",
    "    def display_predictions(model, test_loader, device, num_samples=5):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                # Average multi-channel input for display\n",
    "                input_img = x.cpu().squeeze(0).mean(dim=0).numpy()  # shape (H, W)\n",
    "                # Squeeze target and prediction to get 2D arrays\n",
    "                target_phase = y.cpu().squeeze(0).squeeze(0).numpy()  # shape (H, W)\n",
    "                predicted_phase = y_pred.cpu().squeeze(0).squeeze(0).numpy()  # shape (H, W)\n",
    "                \n",
    "                fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                axs[0].imshow(input_img, cmap='gray')\n",
    "                axs[0].set_title('Input (Avg Intensity)')\n",
    "                axs[1].imshow(target_phase, cmap='gray')\n",
    "                axs[1].set_title('Target Phase Mask (norm)')\n",
    "                axs[2].imshow(predicted_phase, cmap='gray')\n",
    "                axs[2].set_title('Predicted Phase Mask (norm)')\n",
    "                for ax in axs:\n",
    "                    ax.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "                if i+1 >= num_samples:\n",
    "                    break\n",
    "\n",
    "    display_predictions(model, test_loader, device, num_samples=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
